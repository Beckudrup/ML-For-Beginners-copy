{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SMC-AAU-CPH/ML-For-Beginners/blob/main/8-Reinforcement/2-Gym/solution/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATcgDIM5ywmc"
      },
      "source": [
        "## CartPole Skating\n",
        "\n",
        "> **Problem**: If Peter wants to escape from the wolf, he needs to be able to move faster than him. We will see how Peter can learn to skate, in particular, to keep balance, using Q-Learning.\n",
        "\n",
        "First, let's install the gym and import required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4IrfEAlywmd"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "  ! mkdir -p ../images\n",
        "except:\n",
        "  IN_COLAB = False"
      ],
      "metadata": {
        "id": "snZYMsMN1y-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou2C6YBtywme"
      },
      "source": [
        "## Create a cartpole environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjN9rWAJywme"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array_list\")\n",
        "\n",
        "print(env.action_space)\n",
        "print(env.observation_space)\n",
        "print(env.action_space.sample())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-alZBr47ywme"
      },
      "source": [
        "To see how the environment works, let's run a short simulation for 100 steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qB63VAQJywmf"
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "\n",
        "for i in range(100):\n",
        "   env.render()\n",
        "   obs, rew, done, info, _ = env.step(env.action_space.sample())\n",
        "   if done:\n",
        "       break\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ME8Rq54ywmf"
      },
      "source": [
        "During simulation, we need to get observations in order to decide how to act. In fact, `step` function returns us back current observations, reward function, and the `done` flag that indicates whether it makes sense to continue the simulation or not:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17w4ZNudywmf"
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "   env.render()\n",
        "   obs, rew, done, info, _ = env.step(env.action_space.sample())\n",
        "   print(f\"{obs} -> {rew}\")\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5khDJF9bywmf"
      },
      "source": [
        "We can get min and max value of those numbers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJ6Wr3Cvywmf"
      },
      "outputs": [],
      "source": [
        "print(env.observation_space.low)\n",
        "print(env.observation_space.high)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEl6do0Tywmf"
      },
      "source": [
        "## State Discretization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfAXBrikywmf"
      },
      "outputs": [],
      "source": [
        "def discretize(x):\n",
        "    return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(int))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTks2ZBQywmf"
      },
      "source": [
        "Let's also explore other discretization method using bins:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sn8XX0BJywmf"
      },
      "outputs": [],
      "source": [
        "def create_bins(i,num):\n",
        "    return np.arange(num+1)*(i[1]-i[0])/num+i[0]\n",
        "\n",
        "print(\"Sample bins for interval (-5,5) with 10 bins\\n\",create_bins((-5,5),10))\n",
        "\n",
        "ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter\n",
        "nbins = [20,20,10,10] # number of bins for each parameter\n",
        "bins = [create_bins(ints[i],nbins[i]) for i in range(4)]\n",
        "\n",
        "def discretize_bins(x):\n",
        "    return tuple(np.digitize(x[i],bins[i]) for i in range(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAdsbXJ5ywmg"
      },
      "source": [
        "Let's now run a short simulation and observe those discrete environment values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWKkSYjJywmg"
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "   #env.render()\n",
        "   obs, rew, done, info, _ = env.step(env.action_space.sample())\n",
        "   #print(discretize_bins(obs))\n",
        "   print(discretize(obs))\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oarrhJXfywmg"
      },
      "source": [
        "## Q-Table Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQ3mOaS2ywmg"
      },
      "outputs": [],
      "source": [
        "Q = {}\n",
        "actions = (0,1)\n",
        "\n",
        "def qvalues(state):\n",
        "    return [Q.get((state,a),0) for a in actions]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brt6742Rywmg"
      },
      "source": [
        "## Let's Start Q-Learning!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5e8aHvOywmg"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "alpha = 0.3\n",
        "gamma = 0.9\n",
        "epsilon = 0.90"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No0_w26_ywmg"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "def probs(v,eps=1e-4):\n",
        "    v = v-v.min()+eps\n",
        "    v = v/v.sum()\n",
        "    return v\n",
        "\n",
        "Qmax = 0\n",
        "cum_rewards = []\n",
        "rewards = []\n",
        "for epoch in tqdm.tqdm(range(500)):\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    cum_reward=0\n",
        "    # == do the simulation ==\n",
        "    while not done:\n",
        "        s = discretize(obs)\n",
        "        if random.random()<epsilon:\n",
        "            # exploitation - chose the action according to Q-Table probabilities\n",
        "            v = probs(np.array(qvalues(s)))\n",
        "            a = random.choices(actions,weights=v)[0]\n",
        "        else:\n",
        "            # exploration - randomly chose the action\n",
        "            a = np.random.randint(env.action_space.n)\n",
        "\n",
        "        obs, rew, done, info, _ = env.step(a)\n",
        "        cum_reward+=rew\n",
        "        ns = discretize(obs)\n",
        "        Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))\n",
        "    cum_rewards.append(cum_reward)\n",
        "    rewards.append(cum_reward)\n",
        "    # == Periodically print results and calculate average reward ==\n",
        "    if epoch%100==0:\n",
        "        print(f\"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}\")\n",
        "        if np.average(cum_rewards) > Qmax:\n",
        "            Qmax = np.average(cum_rewards)\n",
        "            Qbest = Q\n",
        "        cum_rewards=[]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfF3gejdywmg"
      },
      "source": [
        "## Plotting Training Progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r20YhPIBywmg"
      },
      "outputs": [],
      "source": [
        "plt.plot(rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pzwq7rzywmg"
      },
      "source": [
        "From this graph, it is not possible to tell anything, because due to the nature of stochastic training process the length of training sessions varies greatly. To make more sense of this graph, we can calculate **running average** over series of experiments, let's say 100. This can be done conveniently using `np.convolve`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NumsW02Fywmg"
      },
      "outputs": [],
      "source": [
        "def running_average(x,window):\n",
        "    return np.convolve(x,np.ones(window)/window,mode='valid')\n",
        "\n",
        "plt.plot(running_average(rewards,100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NPOe3FUywmg"
      },
      "source": [
        "## Varying Hyperparameters and Seeing the Result in Action\n",
        "\n",
        "Now it would be interesting to actually see how the trained model behaves. Let's run the simulation, and we will be following the same action selection strategy as during training: sampling according to the probability distribution in Q-Table:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvUCCo42ywmg"
      },
      "outputs": [],
      "source": [
        "obs, _ = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "   s = discretize(obs)\n",
        "   env.render()\n",
        "   v = probs(np.array(qvalues(s)))\n",
        "   a = random.choices(actions,weights=v)[0]\n",
        "   obs,_,done,_,_ = env.step(a)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EttYa5Csywmg"
      },
      "source": [
        "\n",
        "## Saving result to an animated GIF\n",
        "\n",
        "If you want to impress your friends, you may want to send them the animated GIF picture of the balancing pole. To do this, we can invoke `env.render` to produce an image frame, and then save those to animated GIF using PIL library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_Laacyzywmg"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "obs, _ = env.reset()\n",
        "done = False\n",
        "i=0\n",
        "ims = []\n",
        "while not done:\n",
        "   s = discretize(obs)\n",
        "   img=env.render()[0]\n",
        "   ims.append(Image.fromarray(img))\n",
        "   v = probs(np.array([Qbest.get((s,a),0) for a in actions]))\n",
        "   a = random.choices(actions,weights=v)[0]\n",
        "   obs,_,done,_,_ = env.step(a)\n",
        "   i+=1\n",
        "env.close()\n",
        "ims[0].save('../images/cartpole-balance.gif',save_all=True,append_images=ims[1::2],loop=0,duration=5)\n",
        "print(i)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# play this animation\n",
        "from IPython.display import Image, display\n",
        "display(Image(open('../images/cartpole-balance.gif','rb').read()))"
      ],
      "metadata": {
        "id": "IRAiH4q716Jj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "2cb9b8e4ba0e26b1cdc35fa509fd363b1c45cea0b423b3e9d0e061db9bbef286"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}